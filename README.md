# Violence Video Classification Project

Binary video classification: **Violence** vs **NonViolence**.

This repository fine-tunes a pretrained PyTorch video model on the Kaggle dataset **Real Life Violence Situations Dataset** and exports the final model weights in **`.safetensors`** format.  
Evaluation is provided on:
1) a held-out **validation split** of the dataset  
2) **external real videos** (internet videos, 3–5+) via an inference script

---

## Task Requirements Checklist

- ✅ PyTorch model trained on `real-life-violence-situations-dataset`
- ✅ Target accuracy **85%+** achieved (Validation accuracy **97.75%**)
- ✅ Final weights exported as **`.safetensors`**
- ✅ Accuracy checked on:
  - ✅ validation split of the dataset
  - ✅ external videos (3–5+) using `infer.py`
- ✅ GitHub repo contains:
  - training code
  - evaluation code
  - inference code
  - README with implementation details
  - final model weights (`.safetensors`)
  - base model used for fine-tuning (pretrained `r2plus1d_18` from torchvision)

---

## Dataset

**Kaggle:** Real Life Violence Situations Dataset  
https://www.kaggle.com/datasets/mohamedmustafa/real-life-violence-situations-dataset

Expected folder structure after extraction:

data/Real Life Violence Dataset/
Violence/
*.mp4
NonViolence/
*.mp4

markdown

Copy code

The dataset does not provide an official train/val split, so this project creates a deterministic stratified split.

---

## Method / Model

- **Backbone:** `torchvision.models.video.r2plus1d_18` (pretrained on Kinetics-400)
- **Fine-tuning:** replace final FC head for 2 classes
- **Clip sampling:** uniform sampling of `num_frames=16` frames per video
- **Transforms:**
  - resize shorter side to 256
  - crop 224×224
  - train-time random resized crop + horizontal flip
  - validation/inference uses center crop
- **Loss:** CrossEntropyLoss
- **Optimizer:** AdamW
- **AMP:** enabled when CUDA available
- **Export:** `artifacts/model.safetensors`

---

## Validation Split

Created by `prepare_splits.py`:

- **80% train / 20% validation**
- stratified by class
- fixed random seed: **42**
- saved to: `splits.csv`

---

## Results (Validation)

Generated by running:

```bash
python eval.py
Saved to:results/val_report.json

Validation accuracy: 0.9775

Per-class metrics (precision / recall / F1, support):

NonViolence: 0.9751 / 0.9800 / 0.9776 (n=200)

Violence: 0.9799 / 0.9750 / 0.9774 (n=200)

Confusion matrix (rows=true, cols=pred):

lua

Copy code
[[196,   4],
 [  5, 195]]
External Videos (Internet) Test
Inference on external real videos is done with:

bash

Copy code
python infer.py --folder external_videos
Predictions are saved to:

results/external_predictions.json

Predictions (P(Violence))
Video	Predicted label	P(Violence)
NV_2.mp4	NonViolence	0.000041
NV_3.mp4	NonViolence	0.000034
NV_4.mp4	NonViolence	0.000247
NV_5.mp4	NonViolence	0.000247
NV_6.mp4	NonViolence	0.000294
V_3.mp4	Violence	0.985662
V_4.mp4	Violence	0.999871
V_5.mp4	Violence	0.996497

Note: The task asks for “videos from the internet (3–5 videos)”.
If needed, add the source URLs for 3–5 videos here (or in external_videos/README.md) mapping filename → URL → expected label.

Repository Structure
pgsql

Copy code
.
├─ data/                              # dataset location (not committed)
├─ external_videos/                    # 3–5+ external videos (optional to commit)
├─ artifacts/
│  ├─ best.pt                          # best checkpoint (PyTorch)
│  ├─ model.safetensors                # final weights (required)
│  ├─ config.json                      # run configuration
│  └─ labels.json                      # label mapping
├─ results/
│  ├─ val_report.json                  # validation metrics report
│  └─ external_predictions.json        # external video predictions
├─ prepare_splits.py                   # create train/val split
├─ dataset.py                          # dataset / frame sampling / transforms
├─ model.py                            # model definition
├─ train.py                            # training & export to safetensors
├─ eval.py                             # validation evaluation report
└─ infer.py                            # inference on videos/folder
Setup (venv)
1) Create and activate venv
Windows (PowerShell):

PowerShell

Copy code
py -3.10 -m venv .venv
.\.venv\Scripts\Activate.ps1
python -m pip install --upgrade pip
Linux / macOS:

bash

Copy code
python3.10 -m venv .venv
source .venv/bin/activate
python -m pip install --upgrade pip
2) Install PyTorch
Install CUDA-enabled PyTorch for your system from:
https://pytorch.org/get-started/locally/

3) Install remaining dependencies
bash

Copy code
pip install numpy tqdm scikit-learn safetensors opencv-python decord
If decordfails on your platform, the loader can fall back to OpenCV, but decordis recommended for speed and decoding robustness.

How to Run (End-to-End)
1) Prepare split
bash

Copy code
python prepare_splits.py
Creates:splits.csv

2) Train
bash

Copy code
python train.py
Creates:

artifacts/best.pt

artifacts/model.safetensors

artifacts/config.json

artifacts/labels.json

3) Evaluate on validation
bash

Copy code
python eval.py
Creates:

results/val_report.json

4) External reference (3–5+ videos)
Put videos into the program external_videos/, then run it.

bash

Copy code
python infer.py --folder external_videos
Creates:

results/external_predictions.json

Notes
Video decode warnings
Some videos may contain corrupted frames (ffmpeg/h264 warnings).
The dataset loader is designed to be robust and avoid crashing by reading only sampled frames and retrying/falling back when decoding fails.

Model file size on GitHub
If artifacts/model.safetensorsis too large for a standard GitHub push:

Use Git LFS or

upload the model via GitHub Releases and link it in this README.